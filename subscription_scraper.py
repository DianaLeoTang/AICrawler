#!/usr/bin/env python3
"""
é¡¶åˆŠåŒ»å­¦æœºå™¨å­¦ä¹ æ–‡ç« çˆ¬å–å·¥å…· - è®¢é˜…ç‰ˆ
é€‚ç”¨äºæœ‰æœŸåˆŠè®¢é˜…æƒé™çš„æœºæ„ç”¨æˆ·
æ”¯æŒå¤šç§æ•°æ®æºï¼šWeb of Science, Scopus, PubMed, æœŸåˆŠå®˜ç½‘RSS
"""

import requests
import feedparser
import json
import csv
import time
from datetime import datetime
from typing import List, Dict
import xml.etree.ElementTree as ET

class SubscriptionScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.results = []
        
    def scrape_nature_rss(self, journal_name: str, rss_url: str) -> List[Dict]:
        """
        é€šè¿‡Natureç³»åˆ—æœŸåˆŠçš„RSSæºè·å–æ–‡ç« 
        Natureæä¾›å…è´¹çš„RSSè®¢é˜…
        """
        print(f"æ­£åœ¨ä»RSSè·å– {journal_name} çš„æ–‡ç« ...")
        try:
            feed = feedparser.parse(rss_url)
            articles = []
            
            keywords = ['machine learning', 'deep learning', 'artificial intelligence', 
                       'neural network', 'AI', 'ML', 'medical', 'clinical', 'diagnosis']
            
            for entry in feed.entries:
                title = entry.get('title', '').lower()
                summary = entry.get('summary', '').lower()
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›¸å…³å…³é”®è¯
                if any(kw in title or kw in summary for kw in keywords):
                    article = {
                        'title': entry.get('title', ''),
                        'authors': entry.get('author', ''),
                        'journal': journal_name,
                        'pub_date': entry.get('published', ''),
                        'link': entry.get('link', ''),
                        'doi': entry.get('prism_doi', ''),
                        'summary': entry.get('summary', '')[:500]
                    }
                    articles.append(article)
            
            print(f"æ‰¾åˆ° {len(articles)} ç¯‡ç›¸å…³æ–‡ç« ")
            return articles
            
        except Exception as e:
            print(f"RSSè§£æé”™è¯¯: {e}")
            return []
    
    def scrape_science_rss(self, journal_name: str, rss_url: str) -> List[Dict]:
        """
        é€šè¿‡Scienceç³»åˆ—æœŸåˆŠçš„RSSæºè·å–æ–‡ç« 
        """
        print(f"æ­£åœ¨ä»RSSè·å– {journal_name} çš„æ–‡ç« ...")
        try:
            feed = feedparser.parse(rss_url)
            articles = []
            
            keywords = ['machine learning', 'deep learning', 'artificial intelligence', 
                       'neural network', 'AI', 'medical', 'clinical']
            
            for entry in feed.entries:
                title = entry.get('title', '').lower()
                summary = entry.get('summary', '').lower()
                
                if any(kw in title or kw in summary for kw in keywords):
                    article = {
                        'title': entry.get('title', ''),
                        'authors': entry.get('author', ''),
                        'journal': journal_name,
                        'pub_date': entry.get('published', ''),
                        'link': entry.get('link', ''),
                        'doi': entry.get('dc_identifier', ''),
                        'summary': entry.get('summary', '')[:500]
                    }
                    articles.append(article)
            
            print(f"æ‰¾åˆ° {len(articles)} ç¯‡ç›¸å…³æ–‡ç« ")
            return articles
            
        except Exception as e:
            print(f"RSSè§£æé”™è¯¯: {e}")
            return []
    
    def search_pubmed_simple(self, journals: List[str], year: int = 2025) -> List[Dict]:
        """
        ç®€åŒ–çš„PubMedæœç´¢ - ä¸€æ¬¡æ€§æœç´¢å¤šä¸ªæœŸåˆŠ
        """
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        
        # æ„å»ºæœŸåˆŠåˆ—è¡¨æŸ¥è¯¢
        journal_query = ' OR '.join([f'"{j}"[Journal]' for j in journals])
        
        query = f'({journal_query}) AND (machine learning OR deep learning OR artificial intelligence) AND (medical OR clinical OR diagnosis) AND {year}[PDAT]'
        
        print(f"æ­£åœ¨PubMedæœç´¢æ‰€æœ‰æœŸåˆŠ...")
        
        try:
            # æœç´¢
            search_url = f"{base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': query,
                'retmax': 200,  # å¢åŠ åˆ°200ç¯‡
                'retmode': 'json',
                'sort': 'pub_date',
                'mindate': f'{year}/01/01',
                'maxdate': f'{year}/12/31'
            }
            
            response = requests.get(search_url, params=search_params, timeout=30)
            response.raise_for_status()
            search_data = response.json()
            
            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            print(f"æ‰¾åˆ° {len(id_list)} ç¯‡æ–‡ç« ")
            
            if not id_list:
                return []
            
            # è·å–è¯¦æƒ…
            time.sleep(0.5)
            summary_url = f"{base_url}esummary.fcgi"
            summary_params = {
                'db': 'pubmed',
                'id': ','.join(id_list),
                'retmode': 'json'
            }
            
            response = requests.get(summary_url, params=summary_params, timeout=30)
            response.raise_for_status()
            summary_data = response.json()
            
            articles = []
            for pmid, article_data in summary_data.get('result', {}).items():
                if pmid == 'uids':
                    continue
                
                authors = article_data.get('authors', [])
                author_list = ', '.join([a.get('name', '') for a in authors[:5]])
                
                article = {
                    'pmid': pmid,
                    'title': article_data.get('title', ''),
                    'authors': author_list,
                    'journal': article_data.get('fulljournalname', ''),
                    'pub_date': article_data.get('pubdate', ''),
                    'doi': article_data.get('elocationid', ''),
                    'link': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"
                }
                articles.append(article)
            
            return articles
            
        except Exception as e:
            print(f"PubMedæœç´¢é”™è¯¯: {e}")
            return []
    
    def scrape_all_simple(self):
        """
        ç®€åŒ–ç‰ˆçˆ¬å– - ä½¿ç”¨æœ€ç›´æ¥çš„æ–¹æ³•
        """
        print("=" * 70)
        print("å¼€å§‹çˆ¬å–2025å¹´é¡¶åˆŠåŒ»å­¦æœºå™¨å­¦ä¹ ç›¸å…³æ–‡ç«  (ç®€åŒ–ç‰ˆ)")
        print("=" * 70)
        
        # Natureç³»åˆ—RSSæº
        nature_feeds = {
            'Nature': 'http://feeds.nature.com/nature/rss/current',
            'Nature Medicine': 'http://feeds.nature.com/nm/rss/current',
            'Nature Biotechnology': 'http://feeds.nature.com/nbt/rss/current',
            'Nature Methods': 'http://feeds.nature.com/nmeth/rss/current',
            'Nature Machine Intelligence': 'http://feeds.nature.com/natmachintell/rss/current',
        }
        
        # Scienceç³»åˆ—RSSæº
        science_feeds = {
            'Science': 'https://www.science.org/rss/news_current.xml',
            'Science Translational Medicine': 'https://www.science.org/rss/stm_current.xml',
        }
        
        print("\n--- æ–¹æ³•1: RSSè®¢é˜…æº (æœ€å¿«) ---")
        # çˆ¬å–Natureç³»åˆ—
        for journal, rss_url in nature_feeds.items():
            articles = self.scrape_nature_rss(journal, rss_url)
            self.results.extend(articles)
            time.sleep(1)
        
        # çˆ¬å–Scienceç³»åˆ—
        for journal, rss_url in science_feeds.items():
            articles = self.scrape_science_rss(journal, rss_url)
            self.results.extend(articles)
            time.sleep(1)
        
        print("\n--- æ–¹æ³•2: PubMedç»Ÿä¸€æœç´¢ ---")
        all_journals = [
            'Nature', 'Nature Medicine', 'Nature Biotechnology', 'Nature Methods',
            'Science', 'Science Translational Medicine',
            'Cell', 'Cell Systems', 'Cell Reports Medicine'
        ]
        
        pubmed_articles = self.search_pubmed_simple(all_journals, 2025)
        self.results.extend(pubmed_articles)
        
        # å»é‡
        unique_results = []
        seen_titles = set()
        for article in self.results:
            title = article.get('title', '').lower()
            if title and title not in seen_titles and len(title) > 10:
                seen_titles.add(title)
                unique_results.append(article)
        
        self.results = unique_results
        print(f"\nâœ“ æ€»å…±æ‰¾åˆ° {len(self.results)} ç¯‡ç‹¬ç‰¹æ–‡ç« ")
    
    def save_results(self, filename: str = 'medical_ml_articles_2025.csv'):
        """ä¿å­˜ç»“æœ"""
        if not self.results:
            print("æ²¡æœ‰ç»“æœå¯ä¿å­˜")
            return
        
        output_path = f'/mnt/user-data/outputs/{filename}'
        
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['title', 'authors', 'journal', 'pub_date', 'doi', 'pmid', 'link', 'summary']
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            
            writer.writeheader()
            for article in self.results:
                writer.writerow(article)
        
        print(f"\nâœ“ CSVç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        # JSONæ ¼å¼
        json_path = output_path.replace('.csv', '.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ JSONç»“æœå·²ä¿å­˜åˆ°: {json_path}")
        
        return output_path
    
    def generate_report(self):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        if not self.results:
            return
        
        print("\n" + "=" * 70)
        print("ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 70)
        
        # æŒ‰æœŸåˆŠç»Ÿè®¡
        journal_count = {}
        for article in self.results:
            journal = article.get('journal', 'Unknown')
            journal_count[journal] = journal_count.get(journal, 0) + 1
        
        print("\nå„æœŸåˆŠæ–‡ç« æ•°é‡:")
        for journal, count in sorted(journal_count.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {journal}: {count} ç¯‡")
        
        # æ˜¾ç¤ºæœ€æ–°çš„10ç¯‡
        print("\n" + "=" * 70)
        print("æœ€æ–°10ç¯‡æ–‡ç« é¢„è§ˆ:")
        print("=" * 70)
        
        for i, article in enumerate(self.results[:10], 1):
            print(f"\n{i}. {article.get('title', 'N/A')}")
            print(f"   ğŸ“š æœŸåˆŠ: {article.get('journal', 'N/A')}")
            print(f"   âœï¸  ä½œè€…: {article.get('authors', 'N/A')[:80]}...")
            print(f"   ğŸ“… æ—¥æœŸ: {article.get('pub_date', 'N/A')}")
            print(f"   ğŸ”— é“¾æ¥: {article.get('link', 'N/A')}")

def main():
    scraper = SubscriptionScraper()
    scraper.scrape_all_simple()
    scraper.save_results()
    scraper.generate_report()

if __name__ == "__main__":
    main()