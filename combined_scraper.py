#!/usr/bin/env python3
"""
Google Scholar + PubMed è”åˆæ£€ç´¢å·¥å…·
æ£€ç´¢2025å¹´Nature/Science/Cellç³»åˆ—åŒ»å­¦æœºå™¨å­¦ä¹ æ–‡ç« 
å–ä¸¤ä¸ªæ•°æ®åº“ç»“æœçš„å¹¶é›†
"""

import requests
import time
import csv
import json
from typing import List, Dict, Set
from datetime import datetime
import re

class ScholarPubMedScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.pubmed_results = []
        self.scholar_results = []
        self.merged_results = []
        
    def search_pubmed(self, year: int = 2025) -> List[Dict]:
        """
        ä½¿ç”¨PubMed APIè¿›è¡Œæ£€ç´¢
        """
        base_url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/"
        
        # å®šä¹‰æœŸåˆŠåˆ—è¡¨
        journals = [
            'Nature', 'Nature Medicine', 'Nature Biotechnology', 'Nature Methods',
            'Nature Machine Intelligence', 'Nature Biomedical Engineering',
            'Science', 'Science Translational Medicine', 'Science Advances',
            'Cell', 'Cell Systems', 'Cell Reports', 'Cell Reports Medicine'
        ]
        
        # æ„å»ºæŸ¥è¯¢
        journal_query = ' OR '.join([f'"{j}"[Journal]' for j in journals])
        
        query = f'''
        ({journal_query}) 
        AND (machine learning[Title/Abstract] OR deep learning[Title/Abstract] 
             OR artificial intelligence[Title/Abstract] OR neural network[Title/Abstract]
             OR AI[Title/Abstract] OR ML[Title/Abstract])
        AND (medical[Title/Abstract] OR clinical[Title/Abstract] 
             OR diagnosis[Title/Abstract] OR patient[Title/Abstract]
             OR disease[Title/Abstract] OR treatment[Title/Abstract]
             OR healthcare[Title/Abstract])
        AND {year}[PDAT]
        '''
        
        print("=" * 70)
        print("ğŸ“š PubMed æ£€ç´¢ä¸­...")
        print("=" * 70)
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæœç´¢è·å–ID
            search_url = f"{base_url}esearch.fcgi"
            search_params = {
                'db': 'pubmed',
                'term': query,
                'retmax': 500,  # å¢åŠ åˆ°500ç¯‡
                'retmode': 'json',
                'sort': 'pub_date',
                'mindate': f'{year}/01/01',
                'maxdate': f'{year}/12/31'
            }
            
            response = requests.get(search_url, params=search_params, timeout=30)
            response.raise_for_status()
            search_data = response.json()
            
            id_list = search_data.get('esearchresult', {}).get('idlist', [])
            total_count = search_data.get('esearchresult', {}).get('count', 0)
            print(f"âœ“ PubMedæ‰¾åˆ° {total_count} ç¯‡æ–‡ç« ï¼Œæ­£åœ¨è·å–å‰ {len(id_list)} ç¯‡è¯¦æƒ…...")
            
            if not id_list:
                return []
            
            # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡è·å–è¯¦æƒ…ï¼ˆæ¯æ¬¡100ç¯‡ï¼‰
            articles = []
            batch_size = 100
            
            for i in range(0, len(id_list), batch_size):
                batch_ids = id_list[i:i+batch_size]
                print(f"  æ­£åœ¨è·å–ç¬¬ {i+1}-{min(i+batch_size, len(id_list))} ç¯‡...")
                
                time.sleep(0.5)  # APIé™åˆ¶
                
                summary_url = f"{base_url}esummary.fcgi"
                summary_params = {
                    'db': 'pubmed',
                    'id': ','.join(batch_ids),
                    'retmode': 'json'
                }
                
                response = requests.get(summary_url, params=summary_params, timeout=30)
                response.raise_for_status()
                summary_data = response.json()
                
                for pmid, article_data in summary_data.get('result', {}).items():
                    if pmid == 'uids':
                        continue
                    
                    authors = article_data.get('authors', [])
                    author_list = '; '.join([a.get('name', '') for a in authors[:10]])
                    
                    # æå–DOI
                    doi = ''
                    article_ids = article_data.get('articleids', [])
                    for aid in article_ids:
                        if aid.get('idtype') == 'doi':
                            doi = aid.get('value', '')
                            break
                    
                    article = {
                        'pmid': pmid,
                        'title': article_data.get('title', ''),
                        'authors': author_list,
                        'journal': article_data.get('fulljournalname', ''),
                        'pub_date': article_data.get('pubdate', ''),
                        'doi': doi or article_data.get('elocationid', ''),
                        'source': article_data.get('source', ''),
                        'link': f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/",
                        'data_source': 'PubMed'
                    }
                    articles.append(article)
            
            print(f"âœ“ PubMedæ£€ç´¢å®Œæˆï¼Œå…±è·å– {len(articles)} ç¯‡æ–‡ç« \n")
            return articles
            
        except Exception as e:
            print(f"âœ— PubMedæ£€ç´¢é”™è¯¯: {e}\n")
            return []
    
    def search_google_scholar_serpapi(self, year: int = 2025, api_key: str = None) -> List[Dict]:
        """
        ä½¿ç”¨SerpAPIæ£€ç´¢Google Scholarï¼ˆéœ€è¦API keyï¼‰
        """
        if not api_key:
            print("=" * 70)
            print("ğŸ“š Google Scholar æ£€ç´¢ï¼ˆéœ€è¦API keyï¼‰")
            print("=" * 70)
            print("âš ï¸  Google Scholaræ£€ç´¢éœ€è¦API key")
            print("   å¯ä»¥åœ¨ https://serpapi.com æ³¨å†Œè·å–å…è´¹é¢åº¦")
            print("   æˆ–ä½¿ç”¨ä¸‹é¢çš„å¤‡ç”¨æ–¹æ³•\n")
            return []
        
        print("=" * 70)
        print("ğŸ“š Google Scholar æ£€ç´¢ä¸­...")
        print("=" * 70)
        
        # å®šä¹‰æœŸåˆŠ
        journals = [
            'Nature', 'Nature Medicine', 'Nature Biotechnology',
            'Science', 'Science Translational Medicine',
            'Cell', 'Cell Systems', 'Cell Reports Medicine'
        ]
        
        all_articles = []
        
        for journal in journals:
            query = f'source:"{journal}" ("machine learning" OR "deep learning" OR "artificial intelligence") (medical OR clinical OR diagnosis) {year}'
            
            print(f"  æ­£åœ¨æœç´¢ {journal}...")
            
            try:
                params = {
                    'engine': 'google_scholar',
                    'q': query,
                    'api_key': api_key,
                    'num': 20,  # æ¯ä¸ªæœŸåˆŠè·å–20ç¯‡
                    'as_ylo': year,
                    'as_yhi': year
                }
                
                response = requests.get('https://serpapi.com/search', params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                results = data.get('organic_results', [])
                
                for result in results:
                    article = {
                        'title': result.get('title', ''),
                        'authors': result.get('publication_info', {}).get('authors', []),
                        'journal': journal,
                        'pub_date': result.get('publication_info', {}).get('summary', ''),
                        'link': result.get('link', ''),
                        'snippet': result.get('snippet', ''),
                        'data_source': 'Google Scholar'
                    }
                    all_articles.append(article)
                
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                print(f"    âœ— é”™è¯¯: {e}")
                continue
        
        print(f"âœ“ Google Scholaræ£€ç´¢å®Œæˆï¼Œå…±è·å– {len(all_articles)} ç¯‡æ–‡ç« \n")
        return all_articles
    
    def search_google_scholar_manual(self, year: int = 2025) -> List[Dict]:
        """
        Google Scholaræ‰‹åŠ¨æ£€ç´¢æŒ‡å—ï¼ˆæ— éœ€APIï¼‰
        """
        print("=" * 70)
        print("ğŸ“š Google Scholar æ‰‹åŠ¨æ£€ç´¢æŒ‡å—")
        print("=" * 70)
        
        journals = [
            'Nature', 'Nature Medicine', 'Nature Biotechnology', 'Nature Methods',
            'Science', 'Science Translational Medicine',
            'Cell', 'Cell Systems', 'Cell Reports Medicine'
        ]
        
        print("\nğŸ” è¯·åœ¨Google Scholarä¸­ä½¿ç”¨ä»¥ä¸‹æ£€ç´¢å¼ï¼š")
        print("\n" + "=" * 70)
        
        for i, journal in enumerate(journals, 1):
            query = f'source:"{journal}" ("machine learning" OR "deep learning" OR "artificial intelligence") (medical OR clinical OR diagnosis) {year}'
            print(f"\n{i}. {journal}:")
            print(f"   {query}")
        
        print("\n" + "=" * 70)
        print("\nğŸ“‹ æ“ä½œæ­¥éª¤ï¼š")
        print("1. è®¿é—® https://scholar.google.com")
        print("2. å¤åˆ¶ä¸Šé¢çš„æ£€ç´¢å¼åˆ°æœç´¢æ¡†")
        print("3. ç‚¹å‡»æœç´¢ç»“æœå³ä¸‹è§’çš„ 'å¼•ç”¨' â†’ 'BibTeX'")
        print("4. æˆ–ç›´æ¥å¤åˆ¶æ ‡é¢˜ã€ä½œè€…ã€æœŸåˆŠç­‰ä¿¡æ¯")
        print("5. åœ¨æ¸…å/å“ˆä½›æ ¡å›­ç½‘å†…å¯ç›´æ¥çœ‹åˆ°PDFé“¾æ¥")
        
        print("\nğŸ’¡ æç¤ºï¼š")
        print("- Google Scholarç»“æœå¯ä»¥ç”¨å¼•ç”¨ç®¡ç†è½¯ä»¶ï¼ˆZotero/EndNoteï¼‰æ‰¹é‡å¯¼å‡º")
        print("- åœ¨æ ¡å›­ç½‘å†…ä¼šè‡ªåŠ¨æ˜¾ç¤ºå›¾ä¹¦é¦†çš„å…¨æ–‡é“¾æ¥")
        print("- å¯ä»¥è®¾ç½®Google Scholarçš„'å›¾ä¹¦é¦†é“¾æ¥'ä¸ºæ¸…åæˆ–å“ˆä½›")
        
        print("\nâš™ï¸  è®¾ç½®å›¾ä¹¦é¦†é“¾æ¥ï¼š")
        print("1. Google Scholar â†’ è®¾ç½® â†’ å›¾ä¹¦é¦†é“¾æ¥")
        print("2. æœç´¢ 'Tsinghua' æˆ– 'Harvard'")
        print("3. å‹¾é€‰å›¾ä¹¦é¦†ï¼Œä¿å­˜")
        print("4. ä¹‹åæœç´¢ç»“æœä¼šæ˜¾ç¤ºå›¾ä¹¦é¦†å…¨æ–‡é“¾æ¥\n")
        
        return []
    
    def merge_results(self, pubmed_results: List[Dict], scholar_results: List[Dict]) -> List[Dict]:
        """
        åˆå¹¶PubMedå’ŒGoogle Scholarç»“æœï¼Œå»é‡
        """
        print("=" * 70)
        print("ğŸ”„ åˆå¹¶ç»“æœå¹¶å»é‡...")
        print("=" * 70)
        
        # ç”¨äºå»é‡çš„é›†åˆ
        seen_titles = set()
        seen_dois = set()
        merged = []
        
        # å…ˆæ·»åŠ PubMedç»“æœ
        for article in pubmed_results:
            title = article.get('title', '').lower().strip()
            doi = article.get('doi', '').lower().strip()
            
            # æ ‡é¢˜å»é‡
            if title and title not in seen_titles:
                seen_titles.add(title)
                if doi:
                    seen_dois.add(doi)
                merged.append(article)
        
        # å†æ·»åŠ Google Scholarç»“æœï¼ˆè·³è¿‡é‡å¤ï¼‰
        for article in scholar_results:
            title = article.get('title', '').lower().strip()
            
            if title and title not in seen_titles:
                seen_titles.add(title)
                merged.append(article)
        
        print(f"âœ“ PubMedç»“æœ: {len(pubmed_results)} ç¯‡")
        print(f"âœ“ Google Scholarç»“æœ: {len(scholar_results)} ç¯‡")
        print(f"âœ“ åˆå¹¶åï¼ˆå»é‡ï¼‰: {len(merged)} ç¯‡\n")
        
        return merged
    
    def save_results(self, results: List[Dict], filename: str = 'combined_results_2025.csv'):
        """
        ä¿å­˜ç»“æœåˆ°CSVå’ŒJSON
        """
        if not results:
            print("âš ï¸  æ²¡æœ‰ç»“æœå¯ä¿å­˜\n")
            return None
        
        output_path = f'/mnt/user-data/outputs/{filename}'
        
        # ä¿å­˜CSV
        with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
            fieldnames = ['title', 'authors', 'journal', 'pub_date', 'doi', 'pmid', 'link', 'data_source', 'snippet']
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            
            writer.writeheader()
            for article in results:
                writer.writerow(article)
        
        print(f"âœ“ CSVç»“æœå·²ä¿å­˜: {output_path}")
        
        # ä¿å­˜JSON
        json_path = output_path.replace('.csv', '.json')
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ JSONç»“æœå·²ä¿å­˜: {json_path}\n")
        
        return output_path
    
    def generate_report(self, results: List[Dict]):
        """
        ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        """
        if not results:
            return
        
        print("=" * 70)
        print("ğŸ“Š ç»Ÿè®¡æŠ¥å‘Š")
        print("=" * 70)
        
        # æŒ‰æœŸåˆŠç»Ÿè®¡
        journal_count = {}
        source_count = {}
        
        for article in results:
            journal = article.get('journal', 'Unknown')
            source = article.get('data_source', 'Unknown')
            
            journal_count[journal] = journal_count.get(journal, 0) + 1
            source_count[source] = source_count.get(source, 0) + 1
        
        print("\nğŸ“š å„æœŸåˆŠæ–‡ç« æ•°é‡:")
        for journal, count in sorted(journal_count.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {journal}: {count} ç¯‡")
        
        print("\nğŸ” æ•°æ®æ¥æºç»Ÿè®¡:")
        for source, count in sorted(source_count.items(), key=lambda x: x[1], reverse=True):
            print(f"  â€¢ {source}: {count} ç¯‡")
        
        print("\n" + "=" * 70)
        print("ğŸ“„ æœ€æ–°10ç¯‡æ–‡ç« é¢„è§ˆ:")
        print("=" * 70)
        
        for i, article in enumerate(results[:10], 1):
            print(f"\n{i}. {article.get('title', 'N/A')}")
            print(f"   ğŸ“š æœŸåˆŠ: {article.get('journal', 'N/A')}")
            print(f"   âœï¸  ä½œè€…: {article.get('authors', 'N/A')[:80]}...")
            print(f"   ğŸ“… æ—¥æœŸ: {article.get('pub_date', 'N/A')}")
            print(f"   ğŸ”— é“¾æ¥: {article.get('link', 'N/A')}")
            print(f"   ğŸ“Š æ¥æº: {article.get('data_source', 'N/A')}")
    
    def run(self, year: int = 2025, serpapi_key: str = None):
        """
        ä¸»æ‰§è¡Œå‡½æ•°
        """
        print("\n" + "=" * 70)
        print("ğŸš€ Google Scholar + PubMed è”åˆæ£€ç´¢")
        print("=" * 70)
        print(f"æ£€ç´¢å¹´ä»½: {year}")
        print(f"ç›®æ ‡æœŸåˆŠ: Nature/Science/Cell ç³»åˆ—")
        print(f"å…³é”®è¯: æœºå™¨å­¦ä¹  + åŒ»å­¦/ä¸´åºŠ")
        print("=" * 70 + "\n")
        
        # 1. PubMedæ£€ç´¢
        self.pubmed_results = self.search_pubmed(year)
        
        # 2. Google Scholaræ£€ç´¢
        if serpapi_key:
            self.scholar_results = self.search_google_scholar_serpapi(year, serpapi_key)
        else:
            # æä¾›æ‰‹åŠ¨æ£€ç´¢æŒ‡å—
            self.search_google_scholar_manual(year)
            self.scholar_results = []
        
        # 3. åˆå¹¶ç»“æœ
        self.merged_results = self.merge_results(self.pubmed_results, self.scholar_results)
        
        # 4. ä¿å­˜ç»“æœ
        if self.merged_results:
            self.save_results(self.merged_results)
            self.generate_report(self.merged_results)
        
        print("=" * 70)
        print("âœ… æ£€ç´¢å®Œæˆï¼")
        print("=" * 70)

def main():
    scraper = ScholarPubMedScraper()
    
    # è¿è¡Œæ£€ç´¢
    # å¦‚æœæœ‰SerpAPI keyï¼Œå¯ä»¥ä¼ å…¥ï¼šscraper.run(2025, serpapi_key='your_key')
    scraper.run(2025)
    
    print("\nğŸ’¡ æç¤ºï¼š")
    print("- PubMedæ•°æ®å·²è‡ªåŠ¨è·å–")
    print("- Google Scholaréœ€è¦æ‰‹åŠ¨æ£€ç´¢æˆ–ä½¿ç”¨API")
    print("- å»ºè®®ä½¿ç”¨Google Scholarçš„'å¼•ç”¨'åŠŸèƒ½æ‰¹é‡å¯¼å‡º")
    print("- å¯ä»¥ç”¨Zoteroç­‰å·¥å…·ä»Google Scholaræ‰¹é‡å¯¼å…¥\n")

if __name__ == "__main__":
    main()